#!/usr/bin/env python3
"""
æ¼”ç¤ºè„šæœ¬ï¼šå±•ç¤ºå¦‚ä½•ä½¿ç”¨æ‰©å±•çš„æƒ…æŠ¥æº

æœ¬è„šæœ¬æ¼”ç¤º:
1. å¦‚ä½•å•ç‹¬è°ƒç”¨æ–°å¢çš„æ•°æ®æºAPI
2. å¦‚ä½•é…ç½®Web Crawlerä½¿ç”¨ç‰¹å®šæ•°æ®æº
3. å¦‚ä½•æŸ¥çœ‹é‡‡é›†ç»“æœçš„ç»Ÿè®¡ä¿¡æ¯
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))


def demo_api_calls():
    """æ¼”ç¤ºå•ç‹¬è°ƒç”¨å„ä¸ªAPI"""
    print("=" * 80)
    print("ğŸ“š æ¼”ç¤º 1: å•ç‹¬è°ƒç”¨æ–°å¢API")
    print("=" * 80)

    from saads.tools.api_tools import (
        _search_reddit_impl,
        _search_hackernews_impl,
        _search_exploitdb_impl,
    )
    import json

    # 1. Reddit ç¤ºä¾‹
    print("\nğŸ” Reddit æœç´¢ç¤ºä¾‹:")
    print("-" * 80)
    result = _search_reddit_impl("MachineLearning", "LLM", 3)
    if not result.startswith("Error"):
        data = json.loads(result)
        print(f"âœ… æ‰¾åˆ° {len(data)} æ¡ Reddit å¸–å­")
        if data:
            print(f"   ç¤ºä¾‹æ ‡é¢˜: {data[0].get('title', 'N/A')[:60]}...")

    # 2. HackerNews ç¤ºä¾‹
    print("\nğŸ“° HackerNews æœç´¢ç¤ºä¾‹:")
    print("-" * 80)
    result = _search_hackernews_impl("AI security", 3)
    if not result.startswith("Error"):
        data = json.loads(result)
        print(f"âœ… æ‰¾åˆ° {len(data)} æ¡ HackerNews æ•…äº‹")
        if data:
            print(f"   ç¤ºä¾‹æ ‡é¢˜: {data[0].get('title', 'N/A')[:60]}...")

    # 3. Exploit-DB ç¤ºä¾‹
    print("\nğŸ’£ Exploit-DB æœç´¢ç¤ºä¾‹:")
    print("-" * 80)
    result = _search_exploitdb_impl("python", 3)
    if not result.startswith("Error"):
        data = json.loads(result)
        print(f"âœ… æ‰¾åˆ° {len(data)} æ¡æ¼æ´åˆ©ç”¨")
        if data:
            print(f"   ç¤ºä¾‹æ ‡é¢˜: {data[0].get('title', 'N/A')[:60]}...")


async def demo_web_crawler_integration():
    """æ¼”ç¤ºWeb Crawleré›†æˆä½¿ç”¨"""
    print("\n" + "=" * 80)
    print("ğŸ•·ï¸  æ¼”ç¤º 2: Web Crawler é›†æˆä½¿ç”¨")
    print("=" * 80)

    from saads.agents.wp1_1.web_crawler import web_crawler_node

    # æ„é€ æµ‹è¯•çŠ¶æ€
    test_state = {
        "collection_strategy": {
            "keywords": ["LLM security"],
            "target_sources": ["reddit", "hackernews"],  # åªä½¿ç”¨æ–°æº
            "max_per_source": 3,
        },
        "raw_intel": [],
    }

    print("\nğŸ“‹ é…ç½®:")
    print(f"  å…³é”®è¯: {test_state['collection_strategy']['keywords']}")
    print(f"  æ•°æ®æº: {test_state['collection_strategy']['target_sources']}")
    print(f"  æ¯æºæœ€å¤§æ•°: {test_state['collection_strategy']['max_per_source']}")

    print("\nğŸš€ å¼€å§‹é‡‡é›†...")
    result = await web_crawler_node(test_state)

    raw_intel = result.get("raw_intel", [])
    print(f"\nâœ… é‡‡é›†å®Œæˆ! å…±è·å– {len(raw_intel)} æ¡åŸå§‹æƒ…æŠ¥")

    # ç»Ÿè®¡å„æ¥æºæ•°é‡
    from collections import Counter

    sources = Counter(item.get("_source_type", "unknown") for item in raw_intel)
    print("\nğŸ“Š æ¥æºç»Ÿè®¡:")
    for source, count in sources.items():
        print(f"  {source}: {count} æ¡")

    # æ˜¾ç¤ºå‰3æ¡ç¤ºä¾‹
    print("\nğŸ“ æƒ…æŠ¥ç¤ºä¾‹:")
    for idx, item in enumerate(raw_intel[:3], 1):
        print(f"\n  [{idx}] {item.get('title', 'N/A')[:70]}")
        print(f"      æ¥æº: {item.get('_source_type', 'N/A')}")
        print(f"      URL: {item.get('url', 'N/A')[:70]}...")


def demo_supervisor_strategy():
    """æ¼”ç¤ºSupervisorç­–ç•¥é…ç½®"""
    print("\n" + "=" * 80)
    print("ğŸ¯ æ¼”ç¤º 3: Supervisor ç­–ç•¥é…ç½®")
    print("=" * 80)

    from saads.agents.wp1_1.supervisor import CATEGORY_SOURCES

    print("\nå„æ”»å‡»ç±»åˆ«æ¨èçš„æ•°æ®æºé…ç½®:\n")
    for category, sources in CATEGORY_SOURCES.items():
        print(f"ğŸ“Œ {category}:")
        print(f"   æ¨èæ•°æ®æº: {', '.join(sources)}")
        print(f"   æ•°æ®æºæ•°é‡: {len(sources)}")

    print("\nğŸ’¡ æç¤º:")
    print("  - Supervisor ä¼šæ ¹æ®å½“å‰æ”»å‡»æ± è¦†ç›–ç‡åŠ¨æ€é€‰æ‹©æ•°æ®æº")
    print("  - å¯ä»¥åœ¨ supervisor.py ä¸­è°ƒæ•´æ¯ä¸ªç±»åˆ«çš„æ•°æ®æºä¼˜å…ˆçº§")


def demo_data_model():
    """æ¼”ç¤ºæ•°æ®æ¨¡å‹æ›´æ–°"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ æ¼”ç¤º 4: æ•°æ®æ¨¡å‹æ‰©å±•")
    print("=" * 80)

    from saads.models.attack import AttackSource

    print("\næ”¯æŒçš„æƒ…æŠ¥æ¥æºç±»å‹:\n")

    # é€šè¿‡ç±»å‹æ³¨è§£è·å–æ‰€æœ‰æ”¯æŒçš„ç±»å‹
    import typing

    source_types = typing.get_args(AttackSource.__annotations__["type"])

    for idx, source_type in enumerate(source_types, 1):
        emoji = {
            "arxiv": "ğŸ“„",
            "nvd": "ğŸ›¡ï¸",
            "github": "ğŸ™",
            "darkweb": "ğŸ•µï¸",
            "reddit": "ğŸ¤–",
            "hackernews": "ğŸ“°",
            "exploitdb": "ğŸ’£",
            "huggingface": "ğŸ¤—",
            "virustotal": "ğŸ¦ ",
            "alienvault": "ğŸ‘½",
            "blog": "ğŸ“",
            "cve": "ğŸ”’",
            "threat_api": "ğŸŒ",
        }.get(source_type, "ğŸ“Œ")

        status = (
            "ğŸ†•"
            if source_type
            in [
                "reddit",
                "hackernews",
                "exploitdb",
                "huggingface",
                "virustotal",
                "alienvault",
            ]
            else ""
        )

        print(f"  {idx:2d}. {emoji} {source_type:15s} {status}")

    print(f"\nâœ… å…±æ”¯æŒ {len(source_types)} ç§æƒ…æŠ¥æ¥æºç±»å‹")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("ğŸ‰ SAADS æƒ…æŠ¥æºæ‰©å±•åŠŸèƒ½æ¼”ç¤º")
    print("=" * 80)
    print("\næœ¬æ¼”ç¤ºå±•ç¤ºæ–°å¢çš„6ä¸ªæ•°æ®æºå’Œæ‰©å±•åŠŸèƒ½çš„ä½¿ç”¨æ–¹æ³•")
    print("æ¼”ç¤ºå†…å®¹:")
    print("  1. å•ç‹¬è°ƒç”¨æ–°å¢API")
    print("  2. Web Crawler é›†æˆä½¿ç”¨")
    print("  3. Supervisor ç­–ç•¥é…ç½®")
    print("  4. æ•°æ®æ¨¡å‹æ‰©å±•")

    try:
        # æ¼”ç¤º1: APIè°ƒç”¨
        demo_api_calls()

        # æ¼”ç¤º2: Web Crawler (å¼‚æ­¥)
        asyncio.run(demo_web_crawler_integration())

        # æ¼”ç¤º3: Supervisorç­–ç•¥
        demo_supervisor_strategy()

        # æ¼”ç¤º4: æ•°æ®æ¨¡å‹
        demo_data_model()

        print("\n" + "=" * 80)
        print("âœ… æ¼”ç¤ºå®Œæˆ!")
        print("=" * 80)
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("  - è¿è¡Œå®Œæ•´æƒ…æŠ¥é‡‡é›†: python main.py run-wp1-1")
        print("  - è¿è¡Œæµ‹è¯•è„šæœ¬: python tests/scripts/test_all_new_sources.py")
        print("  - æŸ¥çœ‹æ–‡æ¡£: INTELLIGENCE_SOURCES_UPDATE.md")
        print()

    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("   è¿™å¯èƒ½æ˜¯ç”±äº:")
        print("   1. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("   2. ä¾èµ–åŒ…æœªå®‰è£…ï¼ˆè¿è¡Œ: pip install -r requirements.txtï¼‰")
        print("   3. APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
